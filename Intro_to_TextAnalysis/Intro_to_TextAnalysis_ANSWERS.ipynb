{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Regex parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the regex pattern `p` above, print the `set` of unique characters in *Monty Python*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = re.findall(p, document)\n",
    "names = set([x[0] for x in matches])\n",
    "print(names, len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have 84 different characters.\n",
    "\n",
    "Now use the `set` you made above to gather all dialogue into a character `dictionary`, with the keys being the character name and the value being a list of that character's lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# char_dict[\"ARTHUR\"] should give you a list of strings with his dialogue\n",
    "\n",
    "# Solution 1\n",
    "\n",
    "char_dict = {}\n",
    "\n",
    "for name in names:\n",
    "    lines = []\n",
    "    for line in matches:\n",
    "        if name == line[0]:\n",
    "            lines.append(line[1])\n",
    "    char_dict[name] = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution 2 (list comprehension)\n",
    "\n",
    "char_dict = {}\n",
    "\n",
    "for name in names:\n",
    "    char_dict[name] = [line[1] for line in matches if name == line[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution 3 (dictionary comprehension)\n",
    "\n",
    "char_dict = {name: [line[1] for line in re.findall(p, document) if line[0] == name] for name in names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_dict[\"ARTHUR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Removing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function below that takes a string as an argument and returns that string without punctuation or stopwords (HINT: You can get a good start for a list of stopwords here: `from nltk.corpus import stopwords`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rem_punc_stop(text_string):\n",
    "    \n",
    "    from string import punctuation\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    for char in punctuation:\n",
    "        text_string = text_string.replace(char, \"\")\n",
    "\n",
    "    toks = word_tokenize(text_string)\n",
    "    toks_reduced = [x for x in toks if x.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return toks_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: POS Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a frequency distribution for Arthur's parts of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in [item for sublist in tagged_sents for item in sublist])\n",
    "tag_fd.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we look at all characters? Create an empty list `collected_stats` and iterate through `char_dict`, calculate the net polarity of each character, and append a tuple of e.g. `(ARTHUR, 11.45)` back to `collected_stats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collected_stats = []\n",
    "for k in char_dict.keys():\n",
    "    blob = TextBlob(' '.join(char_dict[k]))\n",
    "    net_pol = 0\n",
    "    for sentence in blob.sentences:\n",
    "        pol = sentence.sentiment.polarity\n",
    "        net_pol += pol\n",
    "    collected_stats.append((k, net_pol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `sort` this list of tuples by polarity, and print the list of characters in *Monty Python* according to their sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_stats = sorted(collected_stats, key=lambda x: x[1])\n",
    "for t in sorted_stats:\n",
    "    print(t[0], t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: word2vec\n",
    "\n",
    "Play around with the word2vec model above and try to put into words exactly what the model does, and how one should interpret the results. How would you contrast this with the \"bag of words\" model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
